# Hardware

Os computadores possuem duas partes principais: **Hardware** e **Software**

O Hardware do computador inclui as partes físicas de um computador, como **unidade central de processamento (CPU)**, **monitor de vídeo**, **teclado**, **armazenamento de dados do computador**, **placa gráfica**, **placa de som**, **alto-falantes** e **placa-mãe**.

O Software é o conjunto de instruções que podem ser armazenadas e executadas pelo Hardware. O Hardware é denominado "*hard*" por ser "rígido" em relação às alterações, enquanto o Software é "*soft*" por ser "flexível" e fácil de alterar.

O Hardware normalmente é dirigido pelo Software para executar quaisquer comandos/instruções. Uma combinação entre Hardware e Software forma um sistema de computação utilizável.

## Chips e Transistores

Um transistor é um dispositivo semicondutor usado para amplificar ou alternar sinais eletrônicos e energia elétrica. É composto de material semicondutor geralmente com pelo menos três terminais para conexão a um circuito externo. Uma tensão ou corrente aplicada a um par de terminais do transistor controla a corrente através de outro par de terminais. Hoje, alguns transistores são empacotados individualmente, mas muitos mais são encontrados embutidos em circuitos integrados.

![img](https://raw.githubusercontent.com/the-akira/IntroComp/master/materiais/Hardware/Imagens/1st_transistor.jpeg)

<figure>
    <blockquote>
        <p>Uma réplica do primeiro transistor em funcionamento, um <b>point-contact transistor</b> inventado em 1947.</p>
    </blockquote>
</figure>

O transistor mais amplamente usado é o MOSFET (*metal–oxide–semiconductor field-effect transistor*), também conhecido como transistor MOS, que foi inventado pelo engenheiro egípcio Mohamed Atalla juntamente com o engenheiro coreano Dawon Kahng no Bell Labs em 1959.

O MOSFET é o alicerce fundamental dos modernos dispositivos eletrônicos e é onipresente nos sistemas eletrônicos modernos. Um total estimado de 13 sextilhões de MOSFETs foi fabricado entre 1960 e 2018 (pelo menos 99,9% de todos os transistores), tornando o MOSFET o dispositivo mais amplamente fabricado da história.

A maioria dos transistores é feita de silício muito puro e alguns de germânio, mas alguns outros materiais semicondutores são às vezes usados. Um transistor pode ter apenas um tipo de portador de carga em um *field-effect transistor*, ou pode ter dois tipos de portadores de carga em dispositivos *bipolar junction transistor*. Comparados com o tubo de vácuo, os transistores geralmente são menores e requerem menos energia para operar. 

- Transistor - componente eletrônico essencial
- Transistores são de "estado sólido" - sem partes móveis
- Uma das inveções mais importantes da história da humanidade
- Espécie de "Switch" que podemos ligar / desligar com um sinal elétrico
- Transistores microscópicos são "gravados" em chips de silício
- Os chips podem conter bilhões de transistores
- **Exemplos**: Chips de CPU, chips de Memória

Um **circuito integrado** ou circuito integrado monolítico (também conhecido como IC, chip ou microchip) é um conjunto de circuitos eletrônicos em uma pequena peça plana (ou "chip") de material semicondutor que normalmente é silício. A integração de um grande número de minúsculos **transistores MOS** em um pequeno chip resulta em circuitos que são ordens de magnitude menores, mais rápidas e de menor custo do que aquelas construídas com componentes eletrônicos discretos.

![img](https://raw.githubusercontent.com/the-akira/IntroComp/master/materiais/Hardware/Imagens/MSI_nMOS_Soviet_Chip.jpeg)

<figure>
    <blockquote>
        <p>Um chip soviético MSI nMOS fabricado em 1977, parte de uma calculadora de quatro chips projetada em 1970.</p>
    </blockquote>
</figure>

## Lei de Moore

A lei de Moore é a observação de que o número de transistores em um circuito integrado denso (CI) dobra a cada dois anos. A lei de Moore é uma observação e projeção de uma tendência histórica. É um relacionamento empírico e não uma lei física ou natural.

A observação recebeu o nome de Gordon Moore, co-fundador da Fairchild Semiconductor e antigo CEO e co-fundador da Intel, cujo trabalho de 1965 descreveu uma duplicação a cada ano no número de componentes por circuito integrado.

![img](https://raw.githubusercontent.com/the-akira/IntroComp/master/materiais/Hardware/Imagens/Lei_de_Moore.png)

<figure>
    <blockquote>
        <p>O gráfico acima captura a idéia de que, por dólar, a tecnologia de computadores (não apenas os transistores) fica exponencialmente melhor à medida que o tempo passa. Isso é bastante claro se você considerar o custo ou a capacidade de computadores/smartphones/câmeras.</p>
    </blockquote>
</figure>

Alguns especialistas acreditam que os computadores devem atingir os limites físicos da Lei de Moore em algum momento da década de 2020. As altas temperaturas dos transistores acabariam impossibilitando a criação de circuitos menores. Isso ocorre porque o resfriamento dos transistores requer mais energia do que a quantidade de energia que já passa pelos transistores. Em uma entrevista de 2005, o próprio Moore admitiu que sua lei "não pode continuar para sempre. É a natureza das funções exponenciais", disse ele, "elas acabam atingindo um muro".

A visão de um futuro interminavelmente fortalecido e interconectado traz desafios e benefícios. Os transistores em retração impulsionaram os avanços na computação por mais de meio século, mas em breve engenheiros e cientistas precisarão encontrar outras maneiras de tornar os computadores mais capazes. Em vez de processos físicos, aplicativos e software podem ajudar a melhorar a velocidade e a eficiência dos computadores. A computação em nuvem, a comunicação *wireless*, a Internet of Things (IoT) e a física quântica podem desempenhar um papel no futuro da inovação em tecnologia de computadores.

## Componentes Principais de um Computador

A **unidade central de processamento (CPU)**, **memória de acesso aleatório (RAM)** e o **armazenamento persistente (HD/Flash)** são os elementos principais de um computador e são encontrados em todos eles: **laptops**, **smartphones**, **tablets** e **desktops**.

![img](https://raw.githubusercontent.com/the-akira/IntroComp/master/materiais/Hardware/Imagens/PC.png)

<figure>
    <blockquote>
        <p>A figura acima representa as conexões entre os principais componentes de um Computador.</p>
    </blockquote>
</figure>

### 1. CPU

A **unidade central de processamento (CPU)**, também chamado de processador central ou processador principal, é o circuito eletrônico dentro de um computador que executa instruções que compõem um programa de computador. A CPU executa operações aritméticas básicas, lógicas, de controle e de Input/Output(I/O) especificadas pelas instruções no programa.

- Atua como o cérebro: segue as instruções definidas no código
- Executa computações, por exemplo: adicionar dois números
- Atua em conjunto com RAM e armazenamento persistente
- Gigahertz = 1 bilhão de operações por segundo

A CPU executa as intruções dos códigos, manipulando dados, enquanto os outros componentes possuem uma função mais passiva, como o armazenamento de dados. Quando dizemos que um computador pode "adicionar dois números, um bilhão de vezes por segundo", esse é o CPU. 

#### Gigahertz: Velocidade do CPU

Um gigahertz é 1 bilhão de ciclos por segundo (um megahertz é um milhão de ciclos por segundo). Gigahertz é uma medida de velocidade, muito aproximadamente a taxa que uma CPU pode fazer sua operação mais simples por segundo. O Gigahertz não diz com precisão a rapidez com que uma CPU realiza o trabalho, mas é aproximadamente correlacionada. 

CPUs com mais gigahertz também tendem a ser mais custosos para produzir e consomem mais energia (e, como resultado, emitem mais calor) - um grande desafio é colocar CPUs rápidas em pequenos dispositivos como telefones. A empresa ARM é famosa por produzir chips muito produtivos com potência e calor mínimos. Atualmente, quase todos os telefones celulares usam CPUs ARM.

#### Exemplos

- Executar um script que imprime informações na tela e faz cálculos matemáticos.
- Treinar uma rede neural artificial com um grande conjunto de dados de imagens

### 2. RAM

A **memória de acesso aleatório (RAM)** é uma forma de memória de computador que pode ser lida e alterada em qualquer ordem, normalmente usada para armazenar dados de trabalho e código de máquina. Um dispositivo de memória de acesso aleatório permite que os itens de dados sejam lidos ou gravados quase na mesma quantidade de tempo, independentemente da localização física dos dados na memória. 

A RAM contém circuitos de multiplexação e desmultiplexação, para conectar as linhas de dados ao armazenamento endereçado para leitura ou gravação da entrada. Geralmente, mais de um bit de armazenamento é acessado pelo mesmo endereço.

- Atua como um quadro em branco/bloco de notas
- Bytes de armazenamento temporários em funcionamento
- RAM armazena ambos o código e os dados (temporariamente)
- **Exemplo**: abrir uma imagem no GIMP, os dados da imagem são carregados nos bytes da RAM

**Persistência**: RAM não é persistente. O estado desaparece quando a energia é desligada.

Por exemplo: Quando estamos trabalhando em um documento e a energia é desligada repentinamente, caso não tenhamos salvo esse documento em uma memória persistente, o trabalho será perdido.

RAM é a memória de trabalho, o "bloco de notas" que o computador usa para armazenar código e dados que estão sendo usados ativamente. 

A RAM é efetivamente uma área de armazenamento de bytes sob o controle da CPU. A RAM é relativamente rápida e capaz de recuperar o valor de qualquer byte específico em alguns nanossegundos (1 nanossegundo é 1 bilionésimo de segundo). A outra característica principal da RAM é que ela apenas mantém seu estado enquanto for fornecida com energia: a RAM não é um armazenamento "persistente".

#### Exemplos

- Um programa executando em nossa máquina, o código do programa está armazenado em RAM
- Um programa que manipula imagens: os dados da imagem está em RAM
- Cada aba aberta em um Web Browser ocupa uma certa quantidade de RAM

### 3. Armazenamento Persistente: Hard Drive / Flash Drive

Uma unidade de disco rígido (HDD), disco rígido ou disco fixo é um dispositivo eletromecânico de armazenamento de dados que usa armazenamento magnético para armazenar e recuperar dados digitais usando um ou mais discos rígidos de rotação rápida revestidos com material magnético. Os discos são emparelhados com cabeças magnéticas, geralmente dispostas em um braço atuador em movimento, que lê e grava dados nas superfícies do disco. Os dados são acessados de maneira aleatória, significando que blocos individuais de dados podem ser armazenados e recuperados em qualquer ordem. Hard Drives são um tipo de armazenamento não volátil, mantendo os dados armazenados mesmo quando desligados.

- Armazenamento persistente de bytes
- **Persistente** significa preservado mesmo quando não está energizado
- Disco rígido - armazena bytes como um padrão magnético em um disco giratório
- Os discos rígidos são a principal tecnologia de armazenamento persistente há muito tempo

Porém atualmente a memória Flash está se tornando muito popular.

- **Flash** é uma tecnologia de armazenamento persistente do tipo transistor:
- **"Estado Sólido"** - sem partes móveis
- Também conhecidos como pendrives/SSD's
- O Flash é melhor do que um disco rígido em todos os aspectos: mais rápido, mais confiável, menos energia

Armazenamento persistente - armazenamento de longo prazo para bytes como arquivos e pastas. 

Persistente significa que os bytes são armazenados, mesmo quando a energia é removida. Um laptop pode usar um disco rígido giratório (também conhecido como "disco rígido") para armazenamento persistente de arquivos. Ou poderia usar uma "unidade flash", também conhecida como SSD (Solid State Disk), para armazenar bytes em chips flash. O disco rígido lê e grava padrões magnéticos em um disco de metal giratório para armazenar os bytes, enquanto o flash é "estado sólido": sem partes móveis, apenas chips de silício com pequenos grupos de elétrons para armazenar os bytes. Nos dois casos, o armazenamento é persistente, pois mantém seu estado mesmo quando a energia está desligada.

Uma unidade flash é mais rápida e consome menos energia que um disco rígido. No entanto, por byte, o flash é significativamente mais caro que o armazenamento no disco rígido. O Flash está ficando mais barato, por isso pode assumir nichos à custa dos discos rígidos. O flash é muito mais lento que a RAM, portanto, não é um bom substituto para a RAM.

### Hierarquia de Memória

Na [arquitetura do computador](https://en.wikipedia.org/wiki/Computer_architecture), a hierarquia da memória separa o armazenamento do computador em uma hierarquia baseada no tempo de resposta. Como o tempo de resposta, a complexidade e a capacidade estão relacionados, os níveis também podem ser diferenciados por seu desempenho e tecnologias de controle. A hierarquia de memória afeta o desempenho no projeto arquitetônico do computador, previsões de algoritmos e construções de *programação low level* envolvendo [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference).

![img](https://raw.githubusercontent.com/the-akira/IntroComp/master/materiais/Hardware/Imagens/MemoryHierarchy.png)

<figure>
    <blockquote>
        <p>A figura acima representa o diagrama da hierarquia da memória do computador.</p>
    </blockquote>
</figure>

Projetar para alto desempenho requer considerar as restrições da hierarquia de memória, ou seja, o tamanho e as capacidades de cada componente. Cada um dos vários componentes pode ser visto como parte de uma hierarquia de memórias (m1, m2, ..., mn) em que cada membro **m** é tipicamente menor e mais rápido do que o próximo membro mais alto **m + 1** da hierarquia. Para limitar a espera em níveis mais altos, um nível mais baixo responderá enchendo um buffer e sinalizando para ativar a transferência.

Existem quatro níveis principais de armazenamento:

- Interno - [registradores](https://en.wikipedia.org/wiki/Processor_register) e [cache](https://en.wikipedia.org/wiki/CPU_cache) do processador.
- Principal - a [RAM](https://en.wikipedia.org/wiki/Random-access_memory) do sistema e as placas controladoras.
- Armazenamento de massa on-line - Armazenamento secundário.
- Armazenamento off-line em massa - armazenamento terciário e off-line.

Esta é uma estruturação geral da hierarquia da memória. Muitas outras estruturas são úteis. Por exemplo, um algoritmo de paginação pode ser considerado como um nível de memória virtual ao projetar uma arquitetura de computador e pode-se incluir um nível de [armazenamento nearline](https://en.wikipedia.org/wiki/Nearline_storage) entre armazenamento online e offline.

#### Propriedades das Tecnologias na Hierarquia da Memória

- Adicionar complexidade retarda a hierarquia da memória
- A tecnologia de memória CMOx amplia o espaço Flash na hierarquia de memória
- Uma das principais maneiras de aumentar o desempenho do sistema é minimizar o quão baixo na hierarquia da memória é necessário ir para manipular os dados.
- Latência e largura de banda são duas métricas associadas aos caches. Nenhum deles é uniforme, mas é específico para um determinado componente da hierarquia da memória.
- É difícil prever onde residem os dados na hierarquia da memória.
- A localização na hierarquia de memória determina o tempo necessário para que a pré-busca ocorra.

#### Exemplos

O número de níveis na hierarquia de memória e o desempenho em cada nível aumentaram com o tempo. O tipo de memória ou componentes de armazenamento também mudam historicamente. Por exemplo, a hierarquia de memória de um processador **Intel Haswell Mobile** por volta de 2013 é:

- **Registradores do processador**: o acesso mais rápido possível (geralmente 1 ciclo da CPU). Alguns milhares de bytes de tamanho
- **Cache**:
	- Nível 0 (L0) Cache de microoperações - 6 KiB de tamanho
	- Cache de instrução de nível 1 (L1) - 128 KiB de tamanho
	- Cache de dados de nível 1 (L1) - 128 KiB de tamanho. A melhor velocidade de acesso é de cerca de 700 GiB/segundo
	- Nível 2 (L2) Instrução e dados (compartilhados) - 1 MiB de tamanho. A melhor velocidade de acesso é de cerca de 200 GiB/segundo
	- Cache compartilhado de nível 3 (L3) - 6 MiB de tamanho. A melhor velocidade de acesso é de cerca de 100 GB/segundo
	- Cache compartilhado de nível 4 (L4) - 128 MiB de tamanho. A melhor velocidade de acesso é de cerca de 40 GB/segundo
- **Memória principal (armazenamento primário)**: Gigabytes de tamanho. A melhor velocidade de acesso é de cerca de 10 GB/segundo. No caso de uma máquina [NUMA](https://en.wikipedia.org/wiki/Non-Uniform_Memory_Access), os tempos de acesso podem não ser uniformes
- **Armazenamento em disco (armazenamento secundário)**: Terabytes de tamanho. Em 2017, a melhor velocidade de acesso a partir de um [solid state drive](https://en.wikipedia.org/wiki/Solid-state_drive) do consumidor é de cerca de 2.000 MB/segundo
- **Armazenamento nearline (armazenamento terciário)**: Até exabytes de tamanho. Em 2013, a melhor velocidade de acesso é de cerca de 160 MB/segundo
- **Armazenamento offline**

Os níveis mais baixos da hierarquia - dos discos para baixo - também são conhecidos como armazenamento em camadas. A distinção formal entre armazenamento online, nearline e offline é:

- O armazenamento online está imediatamente disponível para *Input*/*Output*.
- O armazenamento nearline não está imediatamente disponível, mas pode ser disponibilizado online rapidamente, sem intervenção humana.
- O armazenamento offline não está imediatamente disponível e requer alguma intervenção humana para colocá-lo online.

Por exemplo, os discos giratórios sempre ligados estão online, enquanto os discos giratórios que diminuem a rotação, como uma matriz massiva de disco ocioso ([MAID](https://en.wikipedia.org/wiki/MAID)), estão nearline. Mídias removíveis, como cartuchos de fita que podem ser carregados automaticamente, como em uma biblioteca de fitas, são nearline, enquanto os cartuchos que devem ser carregados manualmente estão offline.

A maioria das CPUs modernas são tão rápidas que para a maioria das cargas de trabalho de programa, o gargalo é a [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference) dos acessos à memória e a eficiência do cache e da transferência de memória entre diferentes níveis da hierarquia. Como resultado, a CPU passa grande parte do tempo ociosa, esperando que o *Input*/*Output* de memória seja concluído.

Isso às vezes é chamado de custo de espaço, pois um objeto de memória maior tem mais probabilidade de causar *overflow* em um nível pequeno/rápido e requer o uso de um nível maior/mais lento. A carga resultante no uso da memória é conhecida como pressão (respectivamente pressão de registrador, pressão de cache e pressão de memória(principal)).

Os termos para dados ausentes em um nível superior e que precisam ser buscados em um nível inferior são, respectivamente: [register spilling](https://en.wikipedia.org/wiki/Register_spilling) (devido à pressão do registrador: registrador no cache), [cache misss](https://en.wikipedia.org/wiki/Cache_miss) (cache para a memória principal) e [page fault](https://en.wikipedia.org/wiki/Page_fault) (memória principal para o disco).

Linguagens de programação modernas assumem principalmente dois níveis de memória, memória principal e armazenamento em disco, embora em [linguagem assembly](https://en.wikipedia.org/wiki/Assembly_language) e [inline assemblers](https://en.wikipedia.org/wiki/Inline_assembler) em linguagens como C, os registradores podem ser acessados diretamente. Tirar o máximo proveito da hierarquia de memória requer a cooperação de programadores, hardware e compiladores (bem como suporte subjacente do sistema operacional):

- Os programadores são responsáveis por mover dados entre o disco e a memória por meio de *Intput*/*Output* de arquivo.
- O hardware é responsável por mover dados entre a memória e os caches.
- Os [compiladores de otimização](https://en.wikipedia.org/wiki/Optimizing_compiler) são responsáveis por gerar código que, quando executado, fará com que o hardware use caches e registradores de forma eficiente.

Muitos programadores assumem um nível de memória. Isso funciona bem até que o aplicativo atinja uma barreira de desempenho. Em seguida, a hierarquia da memória será avaliada durante a [refatoração do código](https://en.wikipedia.org/wiki/Code_refactoring).

## Sistema de Arquivos (File System)

Na computação, o File System (frequentemente abreviado como fs) controla como os dados são armazenados e recuperados. Sem um sistema de arquivos, os dados colocados em uma mídia de armazenamento seriam um grande corpo de dados, sem nenhuma maneira de sabermos quando um fragmento de dados termina e quando o próximo inicia. Ao separar os dados em partes e dar um nome a cada parte, os dados são facilmente isolados e identificados. Tomando seu nome da maneira como o sistema de gerenciamento de dados em papel é nomeado, cada grupo de dados é chamado de "arquivo". As regras de estrutura e lógica usadas para gerenciar os grupos de dados e seus nomes são chamadas de "sistema de arquivos".

- O **File System** organiza os bytes de armazenamento persistente: arquivos (files) e diretórios (folders)
- **File**: um nome, um identificador para um bloco de bytes
- **Exemplo**: `photo.png` se refere à 70KB de bytes de dados de uma imagem

![img](https://raw.githubusercontent.com/the-akira/IntroComp/master/materiais/Hardware/Imagens/file_system.png)

<figure>
    <blockquote>
        <p>A figura acima apresenta um diretório de nome <b>projeto</b> contendo três arquivos de extensões diferentes.</p>
    </blockquote>
</figure>

Essencialmente, cada arquivo no sistema de arquivos refere-se a um bloco de bytes; portanto, o nome `photo.png` refere-se a um bloco de 70KB de bytes, que são os dados dessa imagem. O sistema de arquivos em vigor fornece ao usuário um nome (e possivelmente um ícone) para um bloco de bytes de dados e permite que o usuário execute operações nesses dados, como movê-lo ou copiá-lo ou abri-lo com um programa. O sistema de arquivos também rastreia informações sobre os bytes: quantos existem, a hora em que foram modificados pela última vez.

A Microsoft usa o sistema de arquivos NTFS proprietário e o Mac OS X possui o equivalente HFS+ da Apple. Muitos dispositivos (câmeras, MP3 players) usam o antigo sistema de arquivos Microsoft FAT32 em seus cartões de memória flash.

## Graphics Processing Unit (GPU)

Uma **unidade de processamento gráfico (GPU)** é um circuito eletrônico especializado projetado para manipular e alterar rapidamente a memória para acelerar a criação de imagens em um buffer de quadro destinado ao *output* para um dispositivo de exibição. 

As GPUs são usadas em sistemas embarcados, telefones celulares, computadores pessoais, estações de trabalho e consoles de jogos. As GPUs modernas são muito eficientes na manipulação de gráficos de computador e processamento de imagens. Sua estrutura altamente paralela os torna mais eficientes do que as unidades de processamento central (CPU) para algoritmos que processam grandes blocos de dados em paralelo. Em um computador pessoal, uma GPU pode estar presente em uma placa de vídeo ou incorporada na placa-mãe.

O termo "GPU" foi cunhado pela Sony em referência à GPU da Sony projetada pela Toshiba no console PlayStation em 1994. O termo foi então popularizado pela Nvidia em 1999, que comercializou a GeForce 256 como "a primeira GPU do mundo", que foi apresentado como um "processador de chip único com mecanismos integrados de transformação, iluminação, configuração/corte de triângulo e renderização".

![img](https://raw.githubusercontent.com/the-akira/IntroComp/master/materiais/Hardware/Imagens/GPU.png)

<figure>
    <blockquote>
        <p>A figura acima mostra os componentes de uma GPU.</p>
    </blockquote>
</figure>